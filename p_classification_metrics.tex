\chapter{Classification binary metrics}



\section{Binary metrics}

As we are in a binary classification problem, several metrics can be used to assess the performance of the models.

Lets define the following notations:

{\ttfamily
\begin{table}[H]
    \centering
    \begin{tabular}{ll}
        \toprule
        $TP / FP$          &    $true/false\ positives$ \\
        $TN / FN$       &    $true/false\ negatives$  \\
        \bottomrule
    \end{tabular}
\end{table}
}

\textbf{Precision}
\begin{align}
    Precision(TP, FP, TN, FN) = \frac{TP}{TP + FN}
\end{align}

\textbf{Recall}
\begin{align}
    Recall(TP, FP, TN, FN) = \frac{TP}{TP + FP}
\end{align}

\textbf{F-score}

The $F_1$ is a measure of a classifier accuracy, computed as the harmonic mean of precision and recall: $$F_1= 2. \frac{precision.recall}{precision+recall}$$\tabularnewline
This definition assign the same weight to precision and recall but depending on the problem, one can be more valuable than the other.

To handle these constraints, the $F_\beta-score$ is adapted: in consists in the \textit{weighted} harmonic mean of recall and precision, assigning $\beta$ times as much importance to recall as precision: $$F_\beta = (1+\beta^2) \frac{precision.recall}{\beta^2.precision+recall}$$
