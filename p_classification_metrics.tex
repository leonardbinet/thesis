\chapter{Classification binary metrics}



\section{Regular binary metrics}


Evaluation metrics should take into account the class imbalance between churners and non-churners. A greater cost should be associated with false negatives than with false positives: misidentify a potential churner costs far more to a company than identifying a non-churner as churner. Moreover, we are more interested in perfectly identifying those customers who are most likely to churn than perfectly identifying \textit{all} the potential churners.

\subsection{Binary classification metrics}

As we are in a binary classification problem, several metrics can be used to assess the performance of the models.

Lets define some naming/metrics used in the following:

{\ttfamily
\begin{table}[H]
    \centering
    \begin{tabular}{ll}
        \toprule
        $TP / FP$          &    $true/false\ positives$ \\
        $TN / FN$       &    $true/false\ negatives$  \\
        $P$       &    $TP+FN$ \\
        $N$    &    $TN + FP$ \\
        $recall$      &    ${TP} / {P}$ \\
        $precision$        &   ${TP} / {(TP+FP)}$ \\
        \bottomrule
    \end{tabular}
\end{table}
}

Precision, recall and accuracy are often used to measure the classification quality of binary classifiers.


\textbf{F-score}

The $F_1$ is a measure of a classifier accuracy, computed as the harmonic mean of precision and recall: $$F_1= 2. \frac{precision.recall}{precision+recall}$$\tabularnewline
This definition assign the same weight to precision and recall but depending on the problem, one can be more valuable than the other.

To handle these constraints, the $F_\beta-score$ is adapted: in consists in the \textit{weighted} harmonic mean of recall and precision, assigning $\beta$ times as much importance to recall as precision: $$F_\beta = (1+\beta^2) \frac{precision.recall}{\beta^2.precision+recall}$$

\textbf{Calibration plot} 

When dealing with probabilistic classifiers, one of the signs that a suitable classification model has been found is also that predicted probabilities (scores) are well calibrated, that is that a fraction of about $p$ of events with predicted probability $p$ actually occurs. Calibration plot is a method that shows us how well the classifier is calibrated \cite{Calibration}: $$x = true\ probability,\ y=predicted\ probability$$

True probabilities are calculated for (sub)sets of examples with the same predicted score $P(y=1|X) \in [c-\delta, c+\delta]$: $$ p_{true}^c = \frac{P_{sub}^c}{P_{sub}^c + N_{sub}^c}$$ with $P_{sub}^c, N_{sub}^c$ being the proportion on positives and negative examples in a given subset with predicted probabilities $[c-\delta, c+\delta]$. The calibration plot of a perfectly calibrated classifier will be a diagonal.


\subsection{Adaptation to multiclass}