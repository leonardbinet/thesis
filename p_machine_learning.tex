\chapter{Machine learning challenges}
\label{cha:results}




\section{Natural Language Processing}

When dealing with natural language, one of most important common denominator is how we represent words as input to any of our models. How do we transform sentences/words into numerical vectors.

In this section, we will quickly describe Mikolov's Word2Vec models (Continuous Bag of Word, and Skip-Gram), that proposed a new way to build word embeddings based on words coocurrence in a context window.

Then we will explain in more detail recent work from Facebook, that propose new models based on Mikolov's work, allowing the use of sub-word information, and proposing a novel way to compute classification tasks while learning embeddings.

\subsection{Word2Vec models}

Word2vec can utilize either of two model architectures to produce a distributed representation of words: continuous bag-of-words (CBOW) or continuous skip-gram. In the continuous bag-of-words architecture, the model predicts the current word from a window of surrounding context words. The order of context words does not influence prediction (bag-of-words assumption). In the continuous skip-gram architecture, the model uses the current word to predict the surrounding window of context words. The skip-gram architecture weighs nearby context words more heavily than more distant context words.

\subsubsection*{Continuous Bag of Words}
\subsubsection*{Skip-Gram}

\subsection{Fasttext}



Word2Vec treats each word in corpus like an atomic entity and generates a vector for each word. It treats words as the smallest unit to train on. 

Fasttext (which is essentially an extension of word2vec model), treats each word as composed of character ngrams. So the vector for a word is made of the sum of this character n grams. For example the word vector “apple” is a sum of the vectors of the n-grams “<ap”, “app”, ”appl”, ”apple”, ”apple>”, “ppl”, “pple”, ”pple>”, “ple”, ”ple>”, ”le>” (assuming hyperparameters for smallest ngram[minn] is 3 and largest ngram[maxn] is 6). 

This difference manifests as follows.

\begin{itemize}
	\item Generate better word embeddings for rare words ( even if words are rare their character n grams are still shared with other words - hence the embeddings can still be good).
	This is simply because, in word2vec a rare word (e.g. 10 occurrences) has fewer neighbors to be tugged by, in comparison to a word that occurs 100 times - the latter has more neighbor context words and hence is tugged more often resulting in better word vectors.

	\item Out of vocabulary words - they can construct the vector for a word from its character n grams even if word doesn't appear in training corpus. Both Word2vec and Glove can't.
	From a practical usage standpoint, the choice of hyperparamters for generating fasttext embeddings becomes key
	since the training is at character n-gram level, it takes longer to generate fasttext embeddings compared to word2vec - the choice of hyper parameters controlling the minimum and maximum n-gram sizes has a direct bearing on this time.

	\item Simulatenous embedding and classification learning: in Word2Vec, we first build our embeddings, and then use it to translate words in features of low dimensionality for further classification. In fasttext, embedding and classification are different layers of a neural network that are trained simultaneously.
\end{itemize}


\cite[Enriching Word Vectors with Subword Information]{fasttextEnriching}
\cite[Bag of Tricks for Efficient Text Classification]{fasttextTricks}

\subsubsection{Model}
Suppose a previous layer $h$ (taking a vector x of given size in ouput space of size d) as follow:
\begin{align}
	h 
	&= 
	\begin{bmatrix} 
		h_1 \\
		h_2 \\
		\vdots \\
		h_{\textit{dim}}
	\end{bmatrix}\\
	&= A^{\top}x
\end{align}


\begin{align}
	h_j = A^{(j)\top}x
\end{align}

With $A$ being the matrix of weights from input to hidden layer, of size $(\textit{dim}, V)$, ie dimension of vector embeddings $\times$ vocabulary size.
\begin{align}
	A &= \left[\begin{array}{cccc}| & | & | & | \\ A^{(1)} & A^{(2)} & \cdots & A^{(\textit{dim})} \\ | & | & | & | \end{array}\right]\\
	A &= \left[
		\begin{array}{cccc} 
		  	-- & A_{(1)} & -- \\
		  	-- & A_{(2)} & -- \\
		  	&\vdots 	\\
		  	-- & A_{(V)} & --
		\end{array}\right]
\end{align}



Let's denote:

\begin{align}
	s_k  = B^{(k)\top} h = \sum_{j=1}^{\textit{dim}} B^{(k)\top}_j h_j 
\end{align}

we can express $f$ as, $\forall k \in [1, K]$:

\begin{align}
	f_k  = \sigma(s_k) = \sigma( \sum_{j=1}^{d} B^{(k)\top}_j h_j) 
\end{align}

Lets consider the following error on one sample $(x, y)$:

\begin{align}
	E = - \sum_{k=1}^K
  			  	\left\{
				    \begin{array}{ll}
				        \log (f_k) & \mbox{if } y_k =1 \\
				        \log (1 - f_k) & \mbox{if } y_k =0
				    \end{array}
				\right.
\end{align}

This error is relevant since the minimization of this error is equivalent to maximizing the maximum of (log-)likelihood.
Plus, it is interpretable: if the $k^{th}$ component $y_k$ is:
\begin{itemize}
	\item positive ($y_k=1$): then the loss increase if $f_k$ is near 0, and decrease if $f_k$ is near 1.
	\item negative ($y_k=0$): then the loss decrease if $f_k$ is near 0, and increase if $f_k$ is near 1.
\end{itemize}



We can represent it as the following multilayer network:

\begin{tikzpicture}[shorten >=1pt,->,draw=black!50, node distance=\layersep]
    \tikzstyle{every pin edge}=[<-,shorten <=1pt]
    \tikzstyle{neuron}=[circle,fill=black!25,minimum size=17pt,inner sep=0pt]
    \tikzstyle{input neuron}=[neuron, fill=yellow!50];
    \tikzstyle{output neuron}=[neuron, fill=blue!50];
    \tikzstyle{hidden neuron}=[neuron, fill=red!50];
    \tikzstyle{true neuron}=[neuron, fill=green!50];
    \tikzstyle{annot} = [text width=4em, text centered]

    % Draw the input layer nodes
    \foreach \name / \y in {1,...,7}
    % This is the same as writing \foreach \name / \y in {1/1,2/2,3/3,4/4}
        \node[input neuron, pin=left:$x_{\y}$ ] (I-\name) at (0,-\y) {};

    % Draw the hidden layer nodes
    \foreach \name / \y in {1,...,3}
        %\path[yshift=0.5cm] node[hidden neuron] (H-\name) at (\layersep,-\y cm) {};
        \node[hidden neuron] (H-\name) at (\layersep,-1 -\y) {};

    % Draw the output layer node
    % \node[output neuron,pin={[pin edge={->}]right:Output}, right of=H-3] (O) {};

	\foreach \name / \y in {1,...,4}
        \node[output neuron, pin=right:$f_{\y}$ ] (O-\name) at (\layersep*2,-0.5 -\y) {};

    % Draw the true layer node

	\foreach \name / \y in {1,...,4}
        \node[true neuron, pin=right:$y_{\y}$ ] (T-\name) at (\layersep*3,-0.5 -\y) {};


    % Connect every node in the input layer with every node in the
    % hidden layer.
    \foreach \source in {1,...,7}
        \foreach \dest in {1,...,3}
            \path (I-\source) edge (H-\dest);

    % Connect every node in the hidden layer with the output layer
    \foreach \source in {1,...,3}
        \foreach \dest in {1,...,4}
	        \path (H-\source) edge (O-\dest);

    % Annotate the layers
    \node[annot,above of=H-1, node distance=2.5cm] (hl) {Hidden layer: $h$ of size \textit{dim}};
    \node[annot,left of=hl] {Input layer: $x$ (size $V$)};
    \node[annot,right of=hl] (ol) {Output layer: $f$ of size $K$};
    \node[annot,right of=ol] {True layer: $y$ of size $K$};

    \node[annot] (A) at (\layersep/2,-6) {$A$};
    \node[annot] (B) at (\layersep*3/2,-5) {$B$};

\end{tikzpicture}

\subsubsection*{Gradient retropropagation}


\begin{align}
	\frac{ \partial E } { \partial f_k } = 
		\left\{
		    \begin{array}{ll}
		        - \frac{1}{f_k} & \mbox{if } y_k =1 \\
		        \frac{1}{1 - f_k} & \mbox{if } y_k =0
		    \end{array}
		\right.
\end{align}


\begin{align}
	\frac{ \partial E } { \partial s_k } 
		=  
		\frac{ \partial E } { \partial f_k } \cdot \frac{ \partial f_k } { \partial s_k } 
		&=
		\left\{
		    \begin{array}{ll}
		        - \frac{1}{f_k} \cdot f_k (1 - f_k)& \mbox{if } y_k =1 \\
		        \frac{1}{1 - f_k} \cdot f_k (1 - f_k)& \mbox{if } y_k =0
		    \end{array}
		\right. \\
		&=
		\left\{
		    \begin{array}{ll}
		       f_k - 1 & \mbox{if } y_k =1 \\
		       f_k & \mbox{if } y_k =0
		    \end{array}
		\right. \\
		&= f_k - y_k
\end{align}



\begin{align}
	\frac{\partial E}{\partial B_i^{(k)}} 
	= 
	\frac{\partial E}{\partial s_k} \cdot \frac{\partial s_k}{\partial B_i^{(k)}} 
	= 
	h_i (f_k - y_k)
\end{align}


Trick: derivate $E$ in regard to $s_k$:
\begin{align}
	\frac{\partial E}{\partial h_j} 
	&= 
	\sum_{k=1}^K \frac{\partial E}{\partial s_k} \cdot \frac{\partial s_k}{\partial h_j} \\
	&= 
	\sum_{k=1}^K B_j^{(k)} (f_k - y_k)
\end{align}


\begin{align}
	\frac{\partial E}{\partial A_i^{(j)}} 
	&= 
	\frac{\partial E}{\partial h_j} \cdot \frac{\partial h_j}{\partial A_i^{(j)}} \\
	&= 
	x_i \sum_{k=1}^K B_j^{(k)} (f_k - y_k)
\end{align}


\subsubsection*{Gradient descent}

At each sample $(x, y)$, given a learning rate $\mu$, we update weight as follow:

$B$ weights:
\begin{align}
	B_i^{(k)\mbox{new}} \leftarrow B_i^{(k)\mbox{old}} - \mu (h_i (f_k - y_k))
\end{align}

$A$ weights:
\begin{align}
	A_i^{(j)\mbox{new}} \leftarrow A_i^{(j)\mbox{old}} - 
	\mu 
	\left(
		x_i \sum_{k=1}^K B_j^{(k)\mbox{new}} (f_k - y_k) 
	\right)
\end{align}

\subsection{Fasttext parameters implementation}

List of all parameters:
\begin{itemize}
	\item \textit{epoch}
	\item \textit{lr}
	\item \textit{lrUpdateRate}
	\item \textit{dim}
	\item \textit{ws}
	\item \textit{loss}
	\item \textit{neg}
	\item \textit{minCountLabel}
	\item \textit{minCount}
	\item \textit{minn}
	\item \textit{maxn}
	\item \textit{bucket}
	\item \textit{t}
\end{itemize}

\subsection*{Dictionnary}

\subsection*{Learning rate}

Fasttext implementation lets you choose two parameters to control $\mu$ over time:
\begin{itemize}
	\item \textit{lr}: sets $\mu$ at initialization
	\item \textit{lrUpdateRate}: how continuous \textit{vs} per steps \textit{lr} decays
\end{itemize}

The learning rate $\mu$ decrease linearly, from initial given parameter $lr$ to zero.





\pagebreak
\section{Multilabel classification}
\subsection{Review of major algorithms}
\subsubsection{Problem Transformation Approaches}
\subsubsection{Adaptative Approaches}
\subsubsection{Neural network}

\subsection{Multiclass/multilabel scoring metrics}
\subsubsection{Regular binary metrics}


Evaluation metrics should take into account the class imbalance between churners and non-churners. A greater cost should be associated with false negatives than with false positives: misidentify a potential churner costs far more to a company than identifying a non-churner as churner. Moreover, we are more interested in perfectly identifying those customers who are most likely to churn than perfectly identifying \textit{all} the potential churners.

\subsubsection{Binary classification metrics}

As we are in a binary classification problem, several metrics can be used to assess the performance of the models.

Lets define some naming/metrics used in the following:

{\ttfamily
\begin{table}[H]
    \centering
    \begin{tabular}{ll}
        \toprule
        $TP / FP$          &    $true/false\ positives$ \\
        $TN / FN$       &    $true/false\ negatives$  \\
        $P$       &    $TP+FN$ \\
        $N$    &    $TN + FP$ \\
        $recall$      &    ${TP} / {P}$ \\
        $precision$        &   ${TP} / {(TP+FP)}$ \\
        \bottomrule
    \end{tabular}
\end{table}
}

Precision, recall and accuracy are often used to measure the classification quality of binary classifiers.


\textbf{F-score}

The $F_1$ is a measure of a classifier accuracy, computed as the harmonic mean of precision and recall: $$F_1= 2. \frac{precision.recall}{precision+recall}$$\tabularnewline
This definition assign the same weight to precision and recall but depending on the problem, one can be more valuable than the other.

To handle these constraints, the $F_\beta-score$ is adapted: in consists in the \textit{weighted} harmonic mean of recall and precision, assigning $\beta$ times as much importance to recall as precision: $$F_\beta = (1+\beta^2) \frac{precision.recall}{\beta^2.precision+recall}$$

\textbf{Calibration plot} 

When dealing with probabilistic classifiers, one of the signs that a suitable classification model has been found is also that predicted probabilities (scores) are well calibrated, that is that a fraction of about $p$ of events with predicted probability $p$ actually occurs. Calibration plot is a method that shows us how well the classifier is calibrated \cite{Calibration}: $$x = true\ probability,\ y=predicted\ probability$$

True probabilities are calculated for (sub)sets of examples with the same predicted score $P(y=1|X) \in [c-\delta, c+\delta]$: $$ p_{true}^c = \frac{P_{sub}^c}{P_{sub}^c + N_{sub}^c}$$ with $P_{sub}^c, N_{sub}^c$ being the proportion on positives and negative examples in a given subset with predicted probabilities $[c-\delta, c+\delta]$. The calibration plot of a perfectly calibrated classifier will be a diagonal.



\subsubsection{Adaptation to multiclass-multilabel: global / local metrics}



\section{Calibration}

\subsection{Predictions precision/recall estimation}

\section{Invalid/sparse data}

\subsection{Strategies to handle sparse data}
Consider recall rather than precision. The presence of a lot a FN will impact precision, not recall.

\subsection{Iterative approach to bootstrap new classes}
Manually label some products with new classes, then focus validation on these products. The validated suggestions will be used in next classification workflow.

After some iterations, there might be enough occurences to ensure sufficient scores.


\section{Misc}

\subsection{Dive into kind tree-structure}
\subsection{Enforce better calibration}
\subsection{Tuning strategies}
